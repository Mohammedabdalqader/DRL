{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Banana_Windows_x86_64/Banana_Windows_x86_64/Banana.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BananaBrain\n"
     ]
    }
   ],
   "source": [
    "print(brain_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Number of actions: 4\n",
      "States look like: [1.         0.         0.         0.         0.84408134 0.\n",
      " 0.         1.         0.         0.0748472  0.         1.\n",
      " 0.         0.         0.25755    1.         0.         0.\n",
      " 0.         0.74177343 0.         1.         0.         0.\n",
      " 0.25854847 0.         0.         1.         0.         0.09355672\n",
      " 0.         1.         0.         0.         0.31969345 0.\n",
      " 0.        ]\n",
      "States have length: 37\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requiered packages \n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from collections import deque,namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine if I want to train the agent on GPU (if available) or CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our Enviroment\n",
    "env_info = env.reset(train_mode=False)[brain_name] \n",
    "action_size = brain.vector_action_space_size\n",
    "state = env_info.vector_observations[0]\n",
    "state_size = len(state) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model consist of 4 fully connected layer and  output layer \n",
    "\n",
    "class RLAgent(nn.Module):\n",
    "    \n",
    "    def __init__(self,brain,env_info):\n",
    "        \n",
    "        super(RLAgent,self).__init__()\n",
    "        \n",
    "        self.s_size  = len(env_info.vector_observations[0])      # the size of env states which = 37 \n",
    "        self.a_size = brain.vector_action_space_size             # the size of available actions in the env which = 4\n",
    "        self.h1_size     = 32\n",
    "        self.h2_size     = 64\n",
    "        \n",
    "        \n",
    "        self.fc1      = nn.Linear(self.s_size,self.h1_size)        \n",
    "        self.fc2      = nn.Linear(self.h1_size,self.h2_size)     \n",
    "        self.fc3      = nn.Linear(self.h2_size,self.a_size)\n",
    "    \n",
    "    def set_weights(self, weights):\n",
    "        s_size = self.s_size\n",
    "        h1_size = self.h1_size\n",
    "        h2_size = self.h2_size\n",
    "        a_size = self.a_size\n",
    "        # separate the weights for each layer\n",
    "        fc1_end = (s_size*h1_size)+h1_size\n",
    "        fc2_end = fc1_end + (h1_size*h2_size)+h2_size\n",
    "        fc1_W = torch.from_numpy(weights[:s_size*h1_size].reshape(s_size, h1_size))\n",
    "        fc1_b = torch.from_numpy(weights[s_size*h1_size:fc1_end])\n",
    "        fc2_W = torch.from_numpy(weights[fc1_end:fc1_end+(h1_size*h2_size)].reshape(h1_size, h2_size))\n",
    "        fc2_b = torch.from_numpy(weights[fc1_end+(h1_size*h2_size):fc2_end])\n",
    "        fc3_W = torch.from_numpy(weights[fc2_end:fc2_end+(h2_size*a_size)].reshape(h2_size, a_size))\n",
    "        fc3_b = torch.from_numpy(weights[fc2_end+(h2_size*a_size):])\n",
    "        \n",
    "        # set the weights for each layer\n",
    "        self.fc1.weight.data.copy_(fc1_W.view_as(self.fc1.weight.data))\n",
    "        self.fc1.bias.data.copy_(fc1_b.view_as(self.fc1.bias.data))\n",
    "        self.fc2.weight.data.copy_(fc2_W.view_as(self.fc2.weight.data))\n",
    "        self.fc2.bias.data.copy_(fc2_b.view_as(self.fc2.bias.data))\n",
    "        self.fc3.weight.data.copy_(fc3_W.view_as(self.fc3.weight.data))\n",
    "        self.fc3.bias.data.copy_(fc3_b.view_as(self.fc3.bias.data))\n",
    "    \n",
    "    def get_weights_dim(self):\n",
    "        return (self.s_size+1)*self.h1_size + (self.h1_size+1)*self.h2_size + (self.h2_size+1)*self.a_size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.tanh(self.fc3(x))\n",
    "        return x\n",
    "        \n",
    "    def evaluate(self, weights,eps, gamma=1.0, max_t=5000):\n",
    "        self.set_weights(weights)\n",
    "        episode_return = 0.0\n",
    "        env_info = env.reset(train_mode=True)[brain_name]  # reset the environment\n",
    "        state = env_info.vector_observations[0]            # get the current state\n",
    "        for t in range(max_t):\n",
    "            state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "            action = self.forward(state)\n",
    "            # Epsilon-greedy action selection\n",
    "            if random.random() > eps:\n",
    "                action = np.argmax(action.cpu().data.numpy()).astype(np.int32)\n",
    "            else:\n",
    "                action =  random.choice(np.arange(self.a_size))\n",
    "            env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "            next_state = env_info.vector_observations[0]        # get the next state\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            done = env_info.local_done[0]                  # see if episode has finished\n",
    "            \n",
    "            state = next_state\n",
    "            episode_return += reward * math.pow(gamma, t)\n",
    "            if done:\n",
    "                break\n",
    "        return episode_return\n",
    "    \n",
    "agent = RLAgent(brain,env_info).to(device)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10\tAverage Score: 0.60\n",
      "Episode 20\tAverage Score: 0.50\n",
      "Episode 30\tAverage Score: 0.57\n",
      "Episode 40\tAverage Score: 0.53\n",
      "Episode 50\tAverage Score: 0.46\n",
      "Episode 60\tAverage Score: 0.45\n",
      "Episode 70\tAverage Score: 0.40\n",
      "Episode 80\tAverage Score: 0.54\n",
      "Episode 90\tAverage Score: 0.52\n",
      "Episode 100\tAverage Score: 0.55\n",
      "Episode 110\tAverage Score: 0.62\n",
      "Episode 120\tAverage Score: 0.66\n",
      "Episode 130\tAverage Score: 0.87\n",
      "Episode 140\tAverage Score: 1.39\n",
      "Episode 150\tAverage Score: 1.90\n",
      "Episode 160\tAverage Score: 2.40\n",
      "Episode 170\tAverage Score: 3.01\n",
      "Episode 180\tAverage Score: 3.57\n",
      "Episode 190\tAverage Score: 4.21\n",
      "Episode 200\tAverage Score: 5.00\n",
      "Episode 210\tAverage Score: 5.90\n",
      "Episode 220\tAverage Score: 6.65\n",
      "Episode 230\tAverage Score: 7.42\n",
      "Episode 240\tAverage Score: 7.72\n",
      "Episode 250\tAverage Score: 8.03\n",
      "Episode 260\tAverage Score: 8.50\n",
      "Episode 270\tAverage Score: 8.83\n",
      "Episode 280\tAverage Score: 9.23\n",
      "Episode 290\tAverage Score: 9.63\n",
      "Episode 300\tAverage Score: 9.72\n",
      "Episode 310\tAverage Score: 9.64\n",
      "Episode 320\tAverage Score: 9.95\n",
      "Episode 330\tAverage Score: 10.09\n",
      "Episode 340\tAverage Score: 10.48\n",
      "Episode 350\tAverage Score: 10.71\n",
      "Episode 360\tAverage Score: 10.83\n",
      "Episode 370\tAverage Score: 10.91\n",
      "Episode 380\tAverage Score: 10.97\n",
      "Episode 390\tAverage Score: 10.88\n",
      "Episode 400\tAverage Score: 11.18\n",
      "Episode 410\tAverage Score: 11.34\n"
     ]
    }
   ],
   "source": [
    "def cem(n_iterations=500, max_t=1000, gamma=1.0, print_every=10, pop_size=50, elite_frac=0.2, sigma=0.5,\n",
    "        eps_start=1.0,eps_decay=0.99,eps_min=0.01):\n",
    "    \"\"\"PyTorch implementation of a cross-entropy method.\n",
    "        \n",
    "    Params\n",
    "    ======\n",
    "        n_iterations (int): maximum number of training iterations\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        gamma (float): discount rate\n",
    "        print_every (int): how often to print average score (over last 100 episodes)\n",
    "        pop_size (int): size of population at each iteration\n",
    "        elite_frac (float): percentage of top performers to use in update\n",
    "        sigma (float): standard deviation of additive noise\n",
    "    \"\"\"\n",
    "    n_elite=int(pop_size*elite_frac)\n",
    "\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    best_weight = sigma*np.random.randn(agent.get_weights_dim())\n",
    "    eps = eps_start\n",
    "    for i_iteration in range(1, n_iterations+1):\n",
    "        weights_pop = [best_weight + (sigma*np.random.randn(agent.get_weights_dim())) for i in range(pop_size)]\n",
    "        rewards = np.array([agent.evaluate(weights,eps, gamma, max_t) for weights in weights_pop])\n",
    "\n",
    "        elite_idxs = rewards.argsort()[-n_elite:]\n",
    "        elite_weights = [weights_pop[i] for i in elite_idxs]\n",
    "        best_weight = np.array(elite_weights).mean(axis=0)\n",
    "\n",
    "        reward = agent.evaluate(best_weight,eps, gamma=1.0)\n",
    "        scores_deque.append(reward)\n",
    "        scores.append(reward)\n",
    "        eps = max(eps_min, eps_decay*eps) # decrease epsilon\n",
    "\n",
    "        torch.save(agent.state_dict(), 'checkpoint.pth')\n",
    "        \n",
    "        if i_iteration % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_iteration, np.mean(scores_deque)))\n",
    "\n",
    "        if np.mean(scores_deque)>=13.0:\n",
    "            print('\\nEnvironment solved in {:d} iterations!\\tAverage Score: {:.2f}'.format(i_iteration-100, np.mean(scores_deque)))\n",
    "            break\n",
    "    return scores\n",
    "\n",
    "scores = cem()\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
